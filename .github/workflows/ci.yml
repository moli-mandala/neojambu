name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly to catch dependency issues
    - cron: '0 2 * * 0'

env:
  UV_CACHE_DIR: /tmp/.uv-cache

jobs:
  test:
    name: Test Suite
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
      fail-fast: false

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        cache-dependency-glob: "uv.lock"

    - name: Restore uv cache
      uses: actions/cache@v4
      with:
        path: /tmp/.uv-cache
        key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
        restore-keys: |
          uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
          uv-${{ runner.os }}

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Download test database
      run: |
        curl -L https://github.com/moli-mandala/data/releases/latest/download/data.db -o data.db
        echo "Database downloaded: $(du -h data.db)"

    - name: Create database indexes
      run: |
        uv run python -c "
        from sqlalchemy import create_engine, text
        engine = create_engine('sqlite:///data.db')
        with engine.connect() as conn:
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_origin_lemma_id ON lemmas(origin_lemma_id)'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_language_id ON lemmas(language_id)'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_order ON lemmas(\"order\")'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_cognateset ON lemmas(cognateset)'))
            conn.commit()
        print('‚úÖ Database indexes created')
        "

    - name: Run performance tests
      run: |
        uv run python tests/test_performance.py

    - name: Run webapp tests
      run: |
        uv run pytest tests/test_webapp.py -v --tb=short

    - name: Test Flask app startup
      run: |
        timeout 30s uv run python -c "
        from app import app
        print('‚úÖ Flask app imports successfully')
        app.config['TESTING'] = True
        client = app.test_client()
        response = client.get('/')
        assert response.status_code == 200
        print('‚úÖ Flask app responds to requests')
        " || (echo "‚ùå Flask app startup failed" && exit 1)

    - name: Run deployment verification
      run: |
        uv run python scripts/verify_deployment.py

    - name: Minimize uv cache
      run: uv cache prune

  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install uv
      uses: astral-sh/setup-uv@v3

    - name: Install linting tools
      run: |
        uv add --dev ruff black isort mypy

    - name: Run ruff (linting)
      run: |
        uv run ruff check . --output-format=github
      continue-on-error: true

    - name: Run black (formatting check)
      run: |
        uv run black --check --diff .
      continue-on-error: true

    - name: Run isort (import sorting check)
      run: |
        uv run isort --check-only --diff .
      continue-on-error: true

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install uv
      uses: astral-sh/setup-uv@v3

    - name: Install security tools
      run: |
        uv add --dev bandit safety

    - name: Run bandit security scan
      run: |
        uv run bandit -r . -f json -o bandit-report.json
      continue-on-error: true

    - name: Run safety check
      run: |
        uv run safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install uv
      uses: astral-sh/setup-uv@v3

    - name: Install dependencies
      run: uv sync

    - name: Download database
      run: |
        curl -L https://github.com/moli-mandala/data/releases/latest/download/data.db -o data.db

    - name: Setup database indexes
      run: |
        uv run python -c "
        from sqlalchemy import create_engine, text
        engine = create_engine('sqlite:///data.db')
        with engine.connect() as conn:
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_origin_lemma_id ON lemmas(origin_lemma_id)'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_language_id ON lemmas(language_id)'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_order ON lemmas(\"order\")'))
            conn.execute(text('CREATE INDEX IF NOT EXISTS idx_lemmas_cognateset ON lemmas(cognateset)'))
            conn.commit()
        "

    - name: Run comprehensive performance tests
      run: |
        uv run python -c "
        import time
        import json
        from sqlalchemy import create_engine, text
        from app import get_session
        from models import Lemma, Language

        results = {}
        engine = create_engine('sqlite:///data.db')

        # Test 1: Database query performance
        print('üîç Testing database query performance...')
        with engine.connect() as conn:
            start = time.time()
            result = conn.execute(text('SELECT COUNT(*) FROM lemmas WHERE origin_lemma_id IS NOT NULL')).scalar()
            end = time.time()
            results['reflex_count_query'] = {'time': end - start, 'count': result}
            print(f'Reflex count query: {end - start:.4f}s ({result} results)')

            start = time.time()
            result = conn.execute(text('SELECT * FROM lemmas ORDER BY \"order\" LIMIT 100')).fetchall()
            end = time.time()
            results['ordered_query'] = {'time': end - start, 'count': len(result)}
            print(f'Ordered query: {end - start:.4f}s ({len(result)} results)')

        # Test 2: Session management performance
        print('üîç Testing session management...')
        start = time.time()
        session = get_session()
        lemmas = session.query(Lemma).limit(100).all()
        session.close()
        end = time.time()
        results['session_query'] = {'time': end - start, 'count': len(lemmas)}
        print(f'Session query: {end - start:.4f}s ({len(lemmas)} results)')

        # Test 3: Flask app performance
        print('üîç Testing Flask app performance...')
        from app import app
        app.config['TESTING'] = True
        client = app.test_client()

        routes = ['/entries', '/languages', '/reflexes', '/references']
        for route in routes:
            start = time.time()
            response = client.get(route)
            end = time.time()
            results[f'route_{route.replace(\"/\", \"\")}'] = {
                'time': end - start,
                'status': response.status_code
            }
            print(f'Route {route}: {end - start:.4f}s (HTTP {response.status_code})')

        # Performance thresholds
        print('üéØ Checking performance thresholds...')
        thresholds = {
            'reflex_count_query': 0.1,  # Should be < 100ms with index
            'ordered_query': 0.05,      # Should be < 50ms with index
            'session_query': 0.1,       # Should be < 100ms
            'route_entries': 2.0,       # Should be < 2s
            'route_languages': 1.0,     # Should be < 1s
        }

        failures = []
        for test, threshold in thresholds.items():
            if test in results and results[test]['time'] > threshold:
                failures.append(f'{test}: {results[test][\"time\"]:.4f}s > {threshold}s')

        if failures:
            print('‚ùå Performance threshold failures:')
            for failure in failures:
                print(f'  - {failure}')
            exit(1)
        else:
            print('‚úÖ All performance thresholds met!')

        # Save results
        with open('performance-results.json', 'w') as f:
            json.dump(results, f, indent=2)
        "

    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: performance-results.json

  deploy-test:
    name: Deployment Test
    runs-on: ubuntu-latest
    needs: [test, lint]
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install uv
      uses: astral-sh/setup-uv@v3

    - name: Test deployment simulation
      run: |
        echo "üöÄ Simulating Heroku deployment..."
        
        # Simulate release process
        chmod +x release.sh
        export DYNO=web.1
        ./release.sh
        
        # Test that the app can start
        timeout 10s uv run gunicorn app:app --bind 0.0.0.0:8000 --workers 1 --timeout 30 &
        GUNICORN_PID=$!
        
        sleep 5
        
        # Test endpoints
        curl -f http://localhost:8000/ || (echo "‚ùå Health check failed" && kill $GUNICORN_PID && exit 1)
        curl -f http://localhost:8000/entries || (echo "‚ùå Entries endpoint failed" && kill $GUNICORN_PID && exit 1)
        
        kill $GUNICORN_PID
        echo "‚úÖ Deployment simulation successful!"

    - name: Check deployment readiness
      run: |
        echo "üìã Deployment Readiness Checklist:"
        
        # Check required files
        files=(
          "Procfile"
          "runtime.txt" 
          "app.json"
          ".buildpacks"
          "release.sh"
          "verify_deployment.py"
          "pyproject.toml"
          "uv.lock"
        )
        
        for file in "${files[@]}"; do
          if [ -f "$file" ]; then
            echo "‚úÖ $file exists"
          else
            echo "‚ùå $file missing"
            exit 1
          fi
        done
        
        echo "‚úÖ All deployment files present!"
        echo "üéâ Ready for Heroku deployment!"

  report:
    name: Test Report
    runs-on: ubuntu-latest
    needs: [test, lint, security, performance]
    if: always()
    steps:
    - name: Download artifacts
      uses: actions/download-artifact@v4

    - name: Generate report
      run: |
        echo "## üìä CI/CD Pipeline Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.test.result }}" == "success" ]; then
          echo "‚úÖ **Tests**: All passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.lint.result }}" == "success" ]; then
          echo "‚úÖ **Code Quality**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è **Code Quality**: Issues found" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.security.result }}" == "success" ]; then
          echo "‚úÖ **Security**: No issues" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è **Security**: Review required" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.performance.result }}" == "success" ]; then
          echo "‚úÖ **Performance**: All benchmarks passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå **Performance**: Benchmarks failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "üîó **Artifacts**: Check the artifacts section for detailed reports" >> $GITHUB_STEP_SUMMARY